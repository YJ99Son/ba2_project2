{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Business Analytics II – Project II\n",
                "### 2020120083 손영진\n",
                "\n",
                "## Overview\n",
                "This project involves building classification models to predict whether an Airbnb host is a \"Superhost\".\n",
                "The process includes:\n",
                "1. Data Preprocessing (Handling missing values, Encoding, Balancing, Scaling)\n",
                "2. Model Building & Hyperparameter Tuning (Logistic Regression, Decision Tree, Random Forest, MLP, KNN, Naive Bayes)\n",
                "3. Final Prediction on Test Data using the best model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install requirements\n",
                "!pip install -r requirements.txt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.utils import resample\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.neural_network import MLPClassifier\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2. Data Load"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load datasets\n",
                "train_path = 'train_data.csv'\n",
                "test_path = 'test_f25.xlsx'\n",
                "\n",
                "df_train = pd.read_csv(train_path)\n",
                "df_test = pd.read_excel(test_path)\n",
                "\n",
                "print(f\"Train Shape: {df_train.shape}\")\n",
                "print(f\"Test Shape: {df_test.shape}\")\n",
                "df_train.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Target Variable\n",
                "target_col = 'host_is_superhost'\n",
                "\n",
                "# Convert target to binary (t/f -> 1/0)\n",
                "df_train[target_col] = df_train[target_col].map({'t': 1, 'f': 0})\n",
                "\n",
                "# Check for missing values in target and drop them\n",
                "df_train = df_train.dropna(subset=[target_col])\n",
                "print(f\"Train Shape after dropping missing targets: {df_train.shape}\")\n",
                "\n",
                "# Separate Target and Features\n",
                "y = df_train[target_col]\n",
                "X = df_train.drop(columns=[target_col])\n",
                "\n",
                "# Identify categorical and numerical columns\n",
                "cat_cols = X.select_dtypes(include=['object']).columns\n",
                "num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
                "\n",
                "print(f\"Categorical Columns: {len(cat_cols)}\")\n",
                "print(f\"Numerical Columns: {len(num_cols)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Handle Missing Values\n",
                "# For numerical, fill with median\n",
                "for col in num_cols:\n",
                "    median_val = X[col].median()  # Calculate median on Train\n",
                "    X[col] = X[col].fillna(median_val)\n",
                "    if col in df_test.columns:\n",
                "        df_test[col] = df_test[col].fillna(median_val)  # Apply Train median to Test\n",
                "\n",
                "# For categorical, fill with mode\n",
                "for col in cat_cols:\n",
                "    mode_val = X[col].mode()[0]  # Calculate mode on Train\n",
                "    X[col] = X[col].fillna(mode_val)\n",
                "    if col in df_test.columns:\n",
                "        df_test[col] = df_test[col].fillna(mode_val)  # Apply Train mode to Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# One-Hot Encoding\n",
                "X_encoded = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
                "df_test_encoded = pd.get_dummies(df_test, columns=cat_cols, drop_first=True)\n",
                "\n",
                "# Align columns (Ensure train and test have same features)\n",
                "X_encoded, df_test_encoded = X_encoded.align(df_test_encoded, join='left', axis=1, fill_value=0)\n",
                "\n",
                "print(f\"Encoded Train Features Shape: {X_encoded.shape}\")\n",
                "print(f\"Encoded Test Features Shape: {df_test_encoded.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 4. Data Balancing\n",
                "\n",
                "We handle class imbalance using resampling.\n",
                "- **Option**: Set `SAMPLING_METHOD` to `'upsample'` or `'downsample'`.\n",
                "- **Upsampling (Default)**: Replicates minority class samples. **Pros**: Retains all information. **Cons**: Increases dataset size, potential for overfitting if not careful.\n",
                "- **Downsampling**: Removes majority class samples. **Pros**: Faster training. **Cons**: Loss of potentially valuable information."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SAMPLING_METHOD = 'upsample'  # Options: 'upsample', 'downsample'\n",
                "\n",
                "# Combine X and y for resampling\n",
                "train_data = pd.concat([X_encoded, y], axis=1)\n",
                "\n",
                "# Check class distribution\n",
                "print(\"Original Class Distribution:\")\n",
                "print(y.value_counts())\n",
                "\n",
                "# Separate majority and minority classes\n",
                "df_majority = train_data[train_data[target_col] == 0]\n",
                "df_minority = train_data[train_data[target_col] == 1]\n",
                "\n",
                "if SAMPLING_METHOD == 'upsample':\n",
                "    print(\"\\nPerforming Upsampling...\")\n",
                "    df_minority_upsampled = resample(df_minority, \n",
                "                                     replace=True,     # sample with replacement\n",
                "                                     n_samples=len(df_majority),    # to match majority class\n",
                "                                     random_state=42)\n",
                "    df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
                "    \n",
                "elif SAMPLING_METHOD == 'downsample':\n",
                "    print(\"\\nPerforming Downsampling...\")\n",
                "    df_majority_downsampled = resample(df_majority, \n",
                "                                       replace=False,    # sample without replacement\n",
                "                                       n_samples=len(df_minority),    # to match minority class\n",
                "                                       random_state=42)\n",
                "    df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
                "\n",
                "# Display new class counts\n",
                "print(\"\\nBalanced Class Distribution:\")\n",
                "print(df_balanced[target_col].value_counts())\n",
                "\n",
                "# Separate X and y again\n",
                "X_balanced = df_balanced.drop(columns=[target_col])\n",
                "y_balanced = df_balanced[target_col]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 5. Feature Scaling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X_balanced)\n",
                "X_test_scaled = scaler.transform(df_test_encoded)\n",
                "\n",
                "X_scaled = pd.DataFrame(X_scaled, columns=X_balanced.columns)\n",
                "X_test_scaled = pd.DataFrame(X_test_scaled, columns=df_test_encoded.columns)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 6. Model Building & Hyperparameter Tuning\n",
                "\n",
                "### Hyperparameter Tuning Strategy\n",
                "We use `GridSearchCV` to find the optimal hyperparameters for each model. Here is the rationale for the chosen parameter ranges:\n",
                "\n",
                "- **MLP (Neural Network)**:\n",
                "    - `hidden_layer_sizes`: We test different architectures. `(50,)` is a simple model, `(100,)` adds width, and `(50, 50)` adds depth to capture more complex patterns.\n",
                "    - `activation`: `relu` is standard for deep learning, `tanh` can be better for some datasets.\n",
                "    - `alpha`: Regularization parameter to prevent overfitting. We test small values `0.0001` and `0.001`.\n",
                "\n",
                "- **Random Forest & Decision Tree**:\n",
                "    - `max_depth`: Controls tree complexity. `None` allows full growth (risk of overfitting), while `10`, `20` limit it.\n",
                "    - `min_samples_split`: Higher values (e.g., `10`) prevent the model from learning overly specific patterns (noise).\n",
                "    - `n_estimators` (RF only): Number of trees. More trees generally improve performance but increase computation.\n",
                "\n",
                "- **KNN**:\n",
                "    - `n_neighbors`: `3` captures local patterns (sensitive to noise), `7` is smoother.\n",
                "    - `weights`: `distance` gives more weight to closer neighbors, `uniform` treats all equally.\n",
                "\n",
                "- **Logistic Regression**:\n",
                "    - `C`: Inverse of regularization strength. Smaller values (e.g., `0.1`) specify stronger regularization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define models and their hyperparameter grids\n",
                "model_params = {\n",
                "    \"Logistic Regression\": {\n",
                "        \"model\": LogisticRegression(max_iter=1000, random_state=42),\n",
                "        \"params\": {\n",
                "            'C': [0.1, 1, 10]\n",
                "        }\n",
                "    },\n",
                "    \"Decision Tree\": {\n",
                "        \"model\": DecisionTreeClassifier(random_state=42),\n",
                "        \"params\": {\n",
                "            'max_depth': [None, 10, 20, 30],\n",
                "            'min_samples_split': [2, 5, 10]\n",
                "        }\n",
                "    },\n",
                "    \"Random Forest\": {\n",
                "        \"model\": RandomForestClassifier(random_state=42),\n",
                "        \"params\": {\n",
                "            'n_estimators': [50, 100, 200],\n",
                "            'max_depth': [None, 10, 20],\n",
                "            'min_samples_split': [2, 5]\n",
                "        }\n",
                "    },\n",
                "    \"MLP (Neural Network)\": {\n",
                "        \"model\": MLPClassifier(max_iter=1000, random_state=42),\n",
                "        \"params\": {\n",
                "            'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
                "            'activation': ['relu', 'tanh'],\n",
                "            'alpha': [0.0001, 0.001]\n",
                "        }\n",
                "    },\n",
                "    \"KNN\": {\n",
                "        \"model\": KNeighborsClassifier(),\n",
                "        \"params\": {\n",
                "            'n_neighbors': [3, 5, 7],\n",
                "            'weights': ['uniform', 'distance']\n",
                "        }\n",
                "    },\n",
                "    \"Naive Bayes\": {\n",
                "        \"model\": GaussianNB(),\n",
                "        \"params\": {} # Naive Bayes typically doesn't have many hyperparameters to tune\n",
                "    }\n",
                "}\n",
                "\n",
                "best_models = {}\n",
                "results = []\n",
                "\n",
                "print(\"--- Hyperparameter Tuning & Evaluation ---\")\n",
                "\n",
                "for name, mp in model_params.items():\n",
                "    print(f\"Processing {name}...\")\n",
                "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, scoring='accuracy', n_jobs=-1)\n",
                "    clf.fit(X_scaled, y_balanced)\n",
                "    \n",
                "    best_models[name] = clf.best_estimator_\n",
                "    results.append({\n",
                "        'Model': name,\n",
                "        'Best Score': clf.best_score_,\n",
                "        'Best Params': clf.best_params_\n",
                "    })\n",
                "    print(f\"  Best Score: {clf.best_score_:.4f}\")\n",
                "    print(f\"  Best Params: {clf.best_params_}\")\n",
                "\n",
                "# Display results dataframe\n",
                "results_df = pd.DataFrame(results).sort_values(by='Best Score', ascending=False)\n",
                "print(\"\\n--- Model Selection Report ---\")\n",
                "print(results_df)\n",
                "\n",
                "# Select the best performing model overall\n",
                "best_model_name = results_df.iloc[0]['Model']\n",
                "best_model_score = results_df.iloc[0]['Best Score']\n",
                "final_model = best_models[best_model_name]\n",
                "\n",
                "print(f\"\\nSelected Best Model: {best_model_name}\")\n",
                "print(f\"Reason: It achieved the highest cross-validation accuracy of {best_model_score:.4f} among all tested models.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 7. Final Prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict on Test Data\n",
                "final_predictions = final_model.predict(X_test_scaled)\n",
                "\n",
                "# Create submission dataframe\n",
                "submission = pd.DataFrame({\n",
                "    'No': df_test['No'],\n",
                "    'host_is_superhost_pred': final_predictions\n",
                "})\n",
                "\n",
                "# Map 1/0 back to t/f\n",
                "submission['host_is_superhost_pred'] = submission['host_is_superhost_pred'].map({1: 't', 0: 'f'})\n",
                "\n",
                "submission.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save to Excel\n",
                "submission.to_excel('prediction_result.xlsx', index=False)\n",
                "print(\"Prediction saved to prediction_result.xlsx\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 8. Summary\n",
                "\n",
                "**Model Building Process:**\n",
                "1.  **Data Preprocessing**: \n",
                "    -   Target variable `host_is_superhost` was converted to binary.\n",
                "    -   Missing values were filled (median for numerical, mode for categorical).\n",
                "    -   Categorical variables were One-Hot Encoded.\n",
                "    -   Train and Test features were aligned to ensure consistency.\n",
                "    -   Data was balanced using upsampling to address class imbalance.\n",
                "    -   Features were scaled using StandardScaler.\n",
                "\n",
                "2.  **Model Evaluation & Tuning**:\n",
                "    -   We performed GridSearchCV for multiple models: Logistic Regression, Decision Tree, Random Forest, MLP, KNN, and Naive Bayes.\n",
                "    -   Hyperparameters such as hidden layers for MLP, tree depth for Decision Tree/Random Forest, and neighbors for KNN were tuned.\n",
                "    -   5-fold cross-validation was used to ensure robust evaluation.\n",
                "\n",
                "3.  **Prediction**:\n",
                "    -   The model with the highest cross-validation accuracy was selected.\n",
                "    -   Predictions were generated for the test dataset and saved to `prediction_result.xlsx`."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}